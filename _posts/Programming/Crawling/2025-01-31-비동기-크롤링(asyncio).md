---
categories: [Programming, Crawling]      # 카테고리 설정
description: asyncio를 이용한 비동기 크롤링 예제      
date: 2025-01-30 18:00:00 +0900    # 날짜를 정확하게 기록하기 위해
# image:                             # 포스트 이미지 설정
#     path: ""
#     alt: ""
tags: [Python, 병렬성, AsyncIO, 동시크롤링]  # 태그 설정
---

> asyncio를 이용한 비동기 크롤링 예제
{: .prompt-info }

> 1. `ThreadPoolExecutor`를 이용한 비동기 크롤링
> 2. 순차 실행과 병렬 실행의 시간 비교
>     - 순차 실행 시간 : 2.37초
>     - 병렬 실행 시간 : 1.09초
> 3. 병렬 실행 시간이 순차 실행 시간보다 빠름을 확인
> 4. 스레드 실행 시작은 동시에 이루어지지만, 완료는 순차적으로 이루어짐
> 5. `ThreadPoolExecutor` 대신 `ProcessPoolExecutor`를 사용하면 프로세스를 이용한 병렬 실행이 가능
> 6. `ThreadPoolExecutor`는 I/O 바운드 작업에 적합하며, `ProcessPoolExecutor`는 CPU 바운드 작업에 적합

**<u>실습 코드</u>**
```python
import asyncio
import timeit
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import ProcessPoolExecutor
import threading

from bs4 import BeautifulSoup
import requests
import re

urls = [
    "https://www.daum.net",
    "https://naver.com",
    "https://www.google.com/",
    "https://tistory.com",
    "https://ko.wikipedia.org",
    "https://www.yahoo.com",
    "https://www.bing.com",
]


def get_title(url):
    try:
        res = requests.get(url, timeout=5)  # 요청 타임아웃 설정
        res.raise_for_status()  # HTTP 오류 발생 시 예외 처리
        soup = BeautifulSoup(res.text, "html.parser")

        if soup.title and soup.title.string:
            title_cont = re.search(r"\w+", soup.title.string)
            return title_cont.group() if title_cont else soup.title.string
        return "No Title Found"
    except requests.RequestException as e:
        return f"Error: {e}"


async def fetch(url, executor):
    loop = asyncio.get_running_loop()
    print("(Start)Thread Name:", threading.current_thread().name, url)
    res = await loop.run_in_executor(executor, get_title, url)
    print("(Done)Thread Name:", threading.current_thread().name, url)
    return res


async def main():
    executor = ThreadPoolExecutor(max_workers=10)
    # executor = ProcessPoolExecutor(max_workers=10)
    tasks = [asyncio.create_task(fetch(url, executor)) for url in urls]
    results = await asyncio.gather(*tasks)

    print("\nResult:", results)


if __name__ == "__main__":
    print()

    # 순차 실행
    start = timeit.default_timer()
    seq_result = [get_title(url) for url in urls]
    duration = timeit.default_timer() - start
    print("Sequential Result : ", seq_result)
    print("Sequential Total Running Time : ", duration)
    print()

    # 병렬 실행
    start = timeit.default_timer()
    asyncio.run(main())
    duration = timeit.default_timer() - start
    # 총 실행 시간
    print("Parallel Total Running Time : ", duration)

```

**<u>실행 결과</u>**
```
Sequential Result :  ['Daum', 'NAVER', 'Google', '티스토리', '위키백과', 'Yahoo', '검색']
Sequential Total Running Time :  2.3742435830645263

(Start)Thread Name: MainThread https://www.daum.net
(Start)Thread Name: MainThread https://naver.com
(Start)Thread Name: MainThread https://www.google.com/
(Start)Thread Name: MainThread https://tistory.com
(Start)Thread Name: MainThread https://ko.wikipedia.org
(Start)Thread Name: MainThread https://www.yahoo.com
(Start)Thread Name: MainThread https://www.bing.com
(Done)Thread Name: MainThread https://naver.com
(Done)Thread Name: MainThread https://www.bing.com
(Done)Thread Name: MainThread https://tistory.com
(Done)Thread Name: MainThread https://www.daum.net
(Done)Thread Name: MainThread https://www.google.com/
(Done)Thread Name: MainThread https://ko.wikipedia.org
(Done)Thread Name: MainThread https://www.yahoo.com

Result: ['Daum', 'NAVER', 'Google', '티스토리', '위키백과', 'Yahoo', '검색']
Parallel Total Running Time :  1.0987619999796152
```
